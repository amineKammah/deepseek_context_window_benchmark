{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47a25fa",
   "metadata": {},
   "source": [
    "## Defining benchmark functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cedbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import textwrap\n",
    "import re\n",
    "import random\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def a_key_function(s):\n",
    "    if s[0].isnumeric():\n",
    "        return s[::-1]\n",
    "    elif s[0].isupper():\n",
    "        return s.lower()\n",
    "    else:\n",
    "        return s[0]\n",
    "\n",
    "a_key_function_str = \"\"\"\n",
    "def a_key_function(s):\n",
    "    if s[0].isnumeric():\n",
    "        return s[::-1]\n",
    "    elif s[0].isupper():\n",
    "        return s.lower()\n",
    "    else:\n",
    "        return s[0]\n",
    "\"\"\"\n",
    "\n",
    "# For a_key_function(s): checks if string is a palindrome\n",
    "def generate_input_for_a_key_function():\n",
    "    length = random.randint(3, 20)\n",
    "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "\n",
    "\n",
    "\n",
    "def sample_function(item):\n",
    "    s = \"\"\n",
    "    for c in item:\n",
    "        if c.isnumeric():\n",
    "            s += \"a\"\n",
    "        else:\n",
    "            s += c\n",
    "    return s\n",
    "\n",
    "\n",
    "sample_function_str = \"\"\"\n",
    "def sample_function(item):\n",
    "    s = \"\"\n",
    "    for c in item:\n",
    "        if c.isnumeric():\n",
    "            s += \"a\"\n",
    "        else:\n",
    "            s += c\n",
    "    return s\n",
    "\"\"\"\n",
    "\n",
    "# For sample_function(item): returns length of input\n",
    "def generate_input_for_sample_function():\n",
    "    length = random.randint(0, 20)\n",
    "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a4fdb",
   "metadata": {},
   "source": [
    "## Creates output required schema for LLM structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a28776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_schema_no_cot(batch_size, dtype, add_analysis_field=False):\n",
    "    properties = \"{\"\n",
    "    if add_analysis_field:\n",
    "        properties += \"\"\"\n",
    "        \"function_analysis\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Analysis of what the function does\"\n",
    "            }\n",
    "        ,\"\"\"\n",
    "    for i in range(batch_size):\n",
    "        properties += f\"\"\"\n",
    "        \"{i}\": {{\n",
    "            \"type\": \"{dtype}\",\n",
    "            \"description\": \"Function output for input {i}\"\n",
    "            }}\n",
    "        ,\"\"\"\n",
    "\n",
    "    properties = properties[:-1] + \"}\"\n",
    "    required = \", \".join(f'\"{i}\"' for i in range(batch_size))\n",
    "    if add_analysis_field:\n",
    "        required = \"\\\"function_analysis\\\", \" + required\n",
    "\n",
    "    return f\"\"\"{{\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {properties},\n",
    "    \"required\": [{required}],\n",
    "    \"additionalProperties\": false\n",
    "    }}\"\"\"\n",
    "\n",
    "\n",
    "def get_output_schema_structured_cot(batch_size: int, dtype: str) -> str:\n",
    "    # Forces the model to create a function analysis right at the end of the context before \n",
    "    outputs_props = \",\\n\".join(\n",
    "        f'''            \"{i}\": {{\n",
    "                \"type\": \"{dtype}\",\n",
    "                \"description\": \"Function output for input {i}\"\n",
    "            }}''' for i in range(batch_size)\n",
    "    )\n",
    "\n",
    "    outputs_required = \", \".join(f'\"{i}\"' for i in range(batch_size))\n",
    "\n",
    "    # Assemble the full schema\n",
    "    return f\"\"\"{{\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {{\n",
    "            \"function_analysis\": {{\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Analysis of what the function does\"\n",
    "            }},\n",
    "            \"outputs\": {{\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {{\n",
    "{outputs_props}\n",
    "                }},\n",
    "                \"required\": [{outputs_required}],\n",
    "                \"additionalProperties\": false\n",
    "            }}\n",
    "        }},\n",
    "        \"required\": [\"function_analysis\", \"outputs\"],\n",
    "        \"additionalProperties\": false\n",
    "    }}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af319b",
   "metadata": {},
   "source": [
    "## Noisy additional context utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d2e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_level_functions(code_string):\n",
    "    lines = code_string.splitlines()\n",
    "    functions = []\n",
    "    current_func = []\n",
    "    in_function = False\n",
    "    indent_level = None\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if re.match(r'^def\\s+\\w+\\(.*\\):', line):\n",
    "            if in_function:\n",
    "                functions.append(\"\\n\".join(current_func).rstrip())\n",
    "                current_func = []\n",
    "\n",
    "            in_function = True\n",
    "            indent_level = len(line) - len(line.lstrip())\n",
    "            current_func.append(line)\n",
    "\n",
    "        elif in_function:\n",
    "            stripped = line.strip()\n",
    "            if stripped == \"\" or (len(line) - len(line.lstrip()) > indent_level):\n",
    "                current_func.append(line)\n",
    "            else:\n",
    "                functions.append(\"\\n\".join(current_func).rstrip())\n",
    "                current_func = []\n",
    "                in_function = False\n",
    "                indent_level = None\n",
    "\n",
    "                if re.match(r'^def\\s+\\w+\\(.*\\):', line):\n",
    "                    in_function = True\n",
    "                    indent_level = len(line) - len(line.lstrip())\n",
    "                    current_func = [line]\n",
    "\n",
    "    if current_func:\n",
    "        functions.append(\"\\n\".join(current_func).rstrip())\n",
    "\n",
    "    return functions\n",
    "\n",
    "\n",
    "def shuffle_functions_with_injection(functions, new_function=None, depth=None, keep_ratio=1.0):\n",
    "    original_count = len(functions)\n",
    "\n",
    "    if not functions:\n",
    "        print(\"[INFO] No top-level functions found.\")\n",
    "        return \"# No top-level functions found.\\n\"\n",
    "\n",
    "\n",
    "\n",
    "    # Apply keep_ratio\n",
    "    keep_ratio = max(0.0, min(1.0, keep_ratio))\n",
    "    keep_count = max(1, int(len(functions) * keep_ratio)) if keep_ratio > 0 else 0\n",
    "    functions = functions[:keep_count]\n",
    "\n",
    "    print(f\"[INFO] Kept {keep_count} out of {original_count} functions (keep_ratio={keep_ratio}).\")\n",
    "\n",
    "    if new_function and depth is not None:\n",
    "        depth = max(0.0, min(1.0, depth))\n",
    "        insert_index = int(round(depth * len(functions)))\n",
    "        cleaned = textwrap.dedent(new_function).strip()\n",
    "        functions.insert(insert_index, cleaned)\n",
    "        print(f\"[INFO] Injected new function at position {insert_index} (depth={depth}).\")\n",
    "\n",
    "    return '\\n\\n'.join(functions)\n",
    "\n",
    "\n",
    "with open(\"all_funcs.py\", \"r\") as f:\n",
    "    input_code = f.read()\n",
    "\n",
    "random.seed(43)\n",
    "functions = extract_top_level_functions(input_code)\n",
    "random.shuffle(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a707ec5",
   "metadata": {},
   "source": [
    "## Openrouter utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx # Import the asynchronous library\n",
    "import os\n",
    "\n",
    "openrouter_api_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "assert openrouter_api_key is not None\n",
    "\n",
    "\n",
    "async def get_response(output_schema, prompt, model, temperature):\n",
    "    \"\"\"\n",
    "    Sends a non-blocking request to the OpenRouter API using httpx.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {openrouter_api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    json_data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": 16000,\n",
    "        \"response_format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"output_dict\",\n",
    "                \"strict\": True,\n",
    "                \"schema\": output_schema,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Use an async client to perform the request\n",
    "    async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "        response = await client.post(\n",
    "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=json_data,\n",
    "        )\n",
    "        \n",
    "        # This will raise an exception for bad responses (4xx or 5xx)\n",
    "        response.raise_for_status() \n",
    "\n",
    "        return response.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df79b8",
   "metadata": {},
   "source": [
    "## Defining the parameters and running the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a660f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_functions_str = [\n",
    "    a_key_function_str, \n",
    "    sample_function_str,\n",
    "]\n",
    "test_functions = [\n",
    "    a_key_function, \n",
    "    sample_function,\n",
    "]\n",
    "\n",
    "args_generators = [\n",
    "    generate_input_for_a_key_function,\n",
    "    generate_input_for_sample_function,\n",
    "]\n",
    "\n",
    "dtypes = [\n",
    "    \"string\",\n",
    "    \"string\",\n",
    "]\n",
    "\n",
    "depths = [\n",
    "    0.,\n",
    "    0.05, \n",
    "    0.2, \n",
    "    0.5, \n",
    "    0.8,\n",
    "    0.95,\n",
    "    1.0\n",
    "]\n",
    "\n",
    "keep_ratios = [\n",
    "    0.02,\n",
    "    0.2, \n",
    "    0.5, \n",
    "    0.8,\n",
    "    1\n",
    "]\n",
    "\n",
    "input_output_lengths = [\n",
    "    4,\n",
    "    64, \n",
    "    256,\n",
    "    512,\n",
    "]\n",
    "\n",
    "\n",
    "def validate(fn, input_args, llm_result):\n",
    "    return fn(*input_args) == llm_result\n",
    "\n",
    "prompt_with_cot = \"\"\"\n",
    "You are given the following Python functions:\n",
    "\n",
    "----------------function-list----------------\n",
    "{functions}\n",
    "---------------------------------------------\n",
    "\n",
    "Run the function named \"{function_name}\" on each of the following inputs:\n",
    "\n",
    "----------------input-list----------------\n",
    "{input}\n",
    "------------------------------------------\n",
    "\n",
    "Start by finding and thoroughly explaining the target function in very simple words. Then proceed to create the output per input. Each key in the dictionary is an index, and the value is the argument(s) to the function.\n",
    "\n",
    "The output must be a valid JSON dictionary where the key is the input index, and the value is the function output:\n",
    "{{\n",
    "    \"target_function_analysis\": \"what does the target function do\",\n",
    "    \"0\": \"output0\",\n",
    "    \"1\": \"output1\",\n",
    "    ...\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "prompt_without_cot = \"\"\"\n",
    "You are given the following Python functions:\n",
    "\n",
    "----------------function-list----------------\n",
    "{functions}\n",
    "---------------------------------------------\n",
    "\n",
    "Run the function named \"{function_name}\" on each of the following inputs:\n",
    "\n",
    "----------------input-list----------------\n",
    "{input}\n",
    "------------------------------------------\n",
    "\n",
    "Each key in the dictionary is an index, and the value is the argument(s) to the function.\n",
    "\n",
    "The output must be a valid JSON dictionary where the key is the input index, and the value is the function output:\n",
    "{{\n",
    "    \"0\": \"output0\",\n",
    "    \"1\": \"output1\",\n",
    "    ...\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8251fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "# (Assuming other necessary imports and helper functions like get_response, \n",
    "# validate, shuffle_functions_with_injection, etc. are defined elsewhere)\n",
    "\n",
    "\n",
    "async def process_combination(\n",
    "    semaphore, model, fn_info, depth, keep_ratio, num_inputs, functions, prompt, temperature, schema_generator\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a single combination of parameters, with retries. This function\n",
    "    is designed to be run as a concurrent task.\n",
    "    \"\"\"\n",
    "    fn_str, fn_obj, arg_gen, dtype = fn_info\n",
    "\n",
    "    # The semaphore ensures we don't exceed the concurrency limit.\n",
    "    async with semaphore:\n",
    "        # 1. Generate input dictionary\n",
    "        input_dict = {i: arg_gen() for i in range(num_inputs)}\n",
    "\n",
    "        # 2. Prepare function list and prompt\n",
    "        injected_func_str = fn_str.strip()\n",
    "        shuffled_code = shuffle_functions_with_injection(\n",
    "            functions, new_function=injected_func_str, depth=depth, keep_ratio=keep_ratio\n",
    "        )\n",
    "        filled_prompt = prompt.format(\n",
    "            functions=shuffled_code, function_name=fn_obj.__name__, input=input_dict\n",
    "        )\n",
    "\n",
    "        # --- Retry Logic ---\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # 3. Get response from the model\n",
    "                output_schema_ = json.loads(schema_generator(num_inputs, dtype))\n",
    "                raw_response = await get_response(output_schema_, filled_prompt, model, temperature)\n",
    "                output_dict = json.loads(raw_response[\"choices\"][0][\"message\"][\"content\"].strip(\"`json\"))\n",
    "\n",
    "                # 4. Validate each result\n",
    "                success_count = 0\n",
    "                for idx, input_val in input_dict.items():\n",
    "                    llm_output = output_dict[\"outputs\"][str(idx)]\n",
    "                    if validate(fn_obj, (input_val,), llm_output):\n",
    "                        success_count += 1\n",
    "\n",
    "                # If validation completes, calculate and return the result\n",
    "                accuracy = success_count / len(input_dict)\n",
    "                result = {\n",
    "                    \"function\": fn_obj.__name__,\n",
    "                    \"depth\": depth,\n",
    "                    \"keep_ratio\": keep_ratio,\n",
    "                    \"input_size\": num_inputs,\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"usage\": raw_response[\"usage\"],\n",
    "                }\n",
    "\n",
    "                print(\n",
    "                    f\"[INFO] {fn_obj.__name__} | depth={depth} | keep={keep_ratio} | size={num_inputs} | ✅ {success_count}/{len(input_dict)} correct\"\n",
    "                )\n",
    "                return result  # Success, exit the function\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Attempt {attempt + 1}/{max_retries} failed for {fn_obj.__name__} (depth={depth}, keep={keep_ratio}): {e}\")\n",
    "                if attempt + 1 == max_retries:\n",
    "                    print(f\"[ERROR] All retries failed for this combination. Skipping.\")\n",
    "                    # Log the persistent failure before returning\n",
    "                    return {\n",
    "                        \"function\": fn_obj.__name__,\n",
    "                        \"depth\": depth,\n",
    "                        \"keep_ratio\": keep_ratio,\n",
    "                        \"input_size\": num_inputs,\n",
    "                        \"accuracy\": 0.0,\n",
    "                        \"error\": str(e),\n",
    "                    }\n",
    "                else:\n",
    "                    await asyncio.sleep(2)  # Wait 2 seconds before the next attempt\n",
    "    return None # Should not be reached, but good practice\n",
    "\n",
    "\n",
    "async def run_all_combinations_concurrent(model, temperature, prompt, schema_generator):\n",
    "    \"\"\"\n",
    "    Runs all combinations of tests concurrently using a semaphore to limit\n",
    "    the number of parallel requests.\n",
    "    \"\"\"\n",
    "    # Set the maximum number of concurrent tasks\n",
    "    CONCURRENCY_LIMIT = 20\n",
    "    semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
    "    tasks = []\n",
    "    \n",
    "    # Package function-specific details together for easier passing\n",
    "\n",
    "    for keep_ratio in keep_ratios:\n",
    "        # Create a fresh iterator for each keep_ratio loop\n",
    "        fn_details_iterator = zip(test_functions_str, test_functions, args_generators, dtypes)\n",
    "        for fn_info in fn_details_iterator:\n",
    "            for depth in depths:\n",
    "                for num_inputs in input_output_lengths:\n",
    "                    print(f\"Queueing task for: {fn_info[1].__name__}, depth={depth}, keep_ratio={keep_ratio}, num_inputs={num_inputs}\")\n",
    "                    # Create a task for each combination and add it to the list\n",
    "                    task = asyncio.create_task(\n",
    "                        process_combination(\n",
    "                            semaphore, model, fn_info, depth, keep_ratio, num_inputs, functions, prompt, temperature, schema_generator\n",
    "                        )\n",
    "                    )\n",
    "                    tasks.append(task)\n",
    "    # Use asyncio.as_completed wrapped with tqdm to process results as they finish\n",
    "    results = []\n",
    "    for future in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Processing combinations\"):\n",
    "        result = await future\n",
    "        results.append(result)\n",
    "\n",
    "    # Filter out any None results that might occur from unexpected errors\n",
    "    return [res for res in results if res is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ea233",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_deepseek_v3 = await run_all_combinations_concurrent('deepseek/deepseek-chat-v3-0324', temperature=0.0, prompt=prompt_without_cot, schema_generator=get_output_schema_no_cot)\n",
    "results_deepseek_r1 = await run_all_combinations_concurrent('deepseek/deepseek-r1-0528', temperature=0.6, prompt=prompt_without_cot, schema_generator=get_output_schema_no_cot))\n",
    "results_deepseek_v3_cot = await run_all_combinations_concurrent('deepseek/deepseek-chat-v3-0324', temperature=0.0, prompt=prompt_with_cot, schema_generator=get_output_schema_structured_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ef045",
   "metadata": {},
   "source": [
    "## Analysing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824cc884",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results_deepseek_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53759b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "# for r in results + results1 + results2 + results3:\n",
    "for r in results:\n",
    "    # if r[\"function\"] != \"a_key_function\":\n",
    "    if \"error\" not in r:\n",
    "        data.append({\n",
    "            \"distance_from_the_middle\": min(r[\"depth\"], 1 - r[\"depth\"]), \n",
    "            \"depth\": r[\"depth\"], \n",
    "            \"context_length\": r[\"keep_ratio\"],\n",
    "            \"function\": r[\"function\"],\n",
    "            \"prompt_tokens\": r[\"usage\"][\"prompt_tokens\"], \n",
    "            \"completion_tokens\": r[\"usage\"][\"completion_tokens\"], \n",
    "            \"batch_size\": r[\"input_size\"],\n",
    "            \"accuracy\": r[\"accuracy\"]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['prompt_tokens_quant'] = pd.cut(df[\"prompt_tokens\"], bins=10)\n",
    "df['completion_tokens_quant'] = pd.cut(df[\"completion_tokens\"], bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264053ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "parameters = [\"depth\", \"prompt_tokens_quant\", \"completion_tokens_quant\", \"batch_size\", \"context_length\"]\n",
    "\n",
    "for p in parameters:\n",
    "    # 1. Create a new figure and get the axes for each plot\n",
    "    plt.figure(figsize=(12, 7)) # A larger figure is better for multiple lines\n",
    "    ax = plt.gca() # Get Current Axes to plot on\n",
    "\n",
    "    # 2. Plot a separate line for each function\n",
    "    # Group by the parameter (e.g., 'depth') AND 'function', then unstack\n",
    "    # to pivot functions into columns.\n",
    "    per_function_accuracy = df.groupby([p, 'function'])['accuracy'].mean().unstack()\n",
    "    per_function_accuracy.plot(\n",
    "        ax=ax,\n",
    "        marker='o',\n",
    "        linestyle=':',  # Use dotted lines for individual functions\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    # 3. Plot the aggregated line for ALL functions\n",
    "    # Group only by the parameter to get the overall average.\n",
    "    overall_accuracy = df.groupby(p)['accuracy'].mean()\n",
    "    display(per_function_accuracy.assign(overall_accuracy=overall_accuracy))\n",
    "    overall_accuracy.plot(\n",
    "        ax=ax,\n",
    "        label='Overall Average',\n",
    "        color='black',\n",
    "        marker='D',      # Use a different marker (diamond)\n",
    "        linestyle='-',   # Use a solid line for the average\n",
    "        linewidth=2.5\n",
    "    )\n",
    "\n",
    "    # 4. Finalize the plot with titles, labels, and a legend\n",
    "    plt.title(f\"Mean Accuracy vs. {p.replace('_', ' ').title()}\", fontsize=16)\n",
    "    plt.xlabel(p.replace('_', ' ').title(), fontsize=12)\n",
    "    plt.ylabel(\"Mean Accuracy\", fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend(title='Function') # The legend is now crucial\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4d54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = df[['distance_from_the_middle', 'batch_size', 'context_length']]\n",
    "y = df['accuracy']\n",
    "\n",
    "# 2. Split Data into Training and Validation Sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.05)\n",
    "\n",
    "# 3. Normalize the Data\n",
    "# We fit the scaler ONLY on the training data to avoid data leakage\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val) # Apply the same transformation to the validation set\n",
    "\n",
    "# Convert scaled arrays back to pandas DataFrames for clarity\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X.columns, index=X_val.index)\n",
    "\n",
    "\n",
    "# 4. Fit the Linear Regression Model\n",
    "# statsmodels requires manually adding a constant (the intercept)\n",
    "X_train_scaled_const = sm.add_constant(X_train_scaled)\n",
    "\n",
    "# Fit the Ordinary Least Squares (OLS) model\n",
    "model = sm.OLS(y_train, X_train_scaled_const).fit()\n",
    "\n",
    "\n",
    "# 5. Analyze the Factors and Confidence Intervals\n",
    "print(\"## Model Summary\")\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# 6. Evaluate Loss on the Validation Set\n",
    "# Add a constant to the validation features as well\n",
    "X_val_scaled_const = sm.add_constant(X_val_scaled, has_constant='add')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_val_scaled_const)\n",
    "\n",
    "# Calculate the loss (Mean Squared Error)\n",
    "loss = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"## Validation Loss (Mean Squared Error): {loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c37088",
   "metadata": {},
   "source": [
    "## Compare all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba69dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_data(results):\n",
    "    data = []\n",
    "    # for r in results + results1 + results2 + results3:\n",
    "    for r in results:\n",
    "        # if r[\"function\"] != \"a_key_function\":\n",
    "        if \"error\" not in r:\n",
    "            data.append({\n",
    "                \"distance_from_the_middle\": min(r[\"depth\"], 1 - r[\"depth\"]), \n",
    "                \"depth\": r[\"depth\"], \n",
    "                \"context_length\": r[\"keep_ratio\"],\n",
    "                \"function\": r[\"function\"],\n",
    "                \"prompt_tokens\": r[\"usage\"][\"prompt_tokens\"], \n",
    "                \"completion_tokens\": r[\"usage\"][\"completion_tokens\"], \n",
    "                \"batch_size\": r[\"input_size\"],\n",
    "                \"accuracy\": r[\"accuracy\"]\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "dfs = {\n",
    "    \"Deepseek R1\": results_deepseek_r1,\n",
    "    \"Deepseek V3\": results_deepseek_v3,\n",
    "    \"Deepseek V3 with specified output\": results_deepseek_v3_cot\n",
    "}\n",
    "\n",
    "parameters = [\n",
    "    \"depth\",\n",
    "    \"batch_size\",\n",
    "    \"context_length\",\n",
    "]\n",
    "\n",
    "for p in parameters:\n",
    "    # --------------------------------------------------------------\n",
    "    # 1⃣  Build a table of overall-average accuracy for this parameter\n",
    "    param_df = pd.concat(\n",
    "        [\n",
    "            data.groupby(p)[\"accuracy\"].mean().rename(label)\n",
    "            for label, data in dfs.items()\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    param_df.index.name = p  # nice row label\n",
    "    print(param_df.to_markdown())        # show the table right above the plot\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 2⃣  Plot the three overall-average lines\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    param_df.plot(\n",
    "        ax=ax,\n",
    "        marker=\"o\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"Overall Mean Accuracy vs. {p.replace('_', ' ').title()}\", fontsize=14)\n",
    "    ax.set_xlabel(p.replace('_', ' ').title(), fontsize=12)\n",
    "    ax.set_ylabel(\"Mean Accuracy\", fontsize=12)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.legend(title=\"Dataset\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b362de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
