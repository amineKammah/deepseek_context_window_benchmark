# LLM Long-Context Benchmark

This repository contains the Python code used to run the "needle in a haystack" benchmarks for the article "Decoding Context Windows: Benchmarking DeepSeek ability to handle 128k tokens."

The scripts are designed to test a large language model's ability to locate and execute a specific function within a large, distracting context.

---

## üî¨ How It Works

1.  **The "Needle"**: Two simple, custom Python functions with obscure names are used as the target instructions. These functions perform tricky character-level string manipulations.

2.  **The "Haystack"**: A large file of unrelated Python code serves as the distracting context, totaling up to 35,000 tokens.

3.  **The Test**: The main script programmatically embeds one of the "needle" functions into the "haystack" at a specified position and length.

4.  **The Prompt**: A prompt is constructed that includes the combined "haystack" and "needle," along with a batch of inputs for the target function to process.

5.  **Validation**: The LLM's output for each input is compared against the actual result generated by executing the original Python function. If the outputs match exactly, the result is marked as correct.

---

## ‚öôÔ∏è Configuration

The benchmark was run by varying three key parameters to generate the 280 test runs per model:

* `context_length`: The size of the "haystack" (distracting code), ranging from 700 to 35,000 tokens.
* `depth`: The position of the "needle" function within the haystack, from 0% (start) to 100% (end).
* `batch_size`: The number of function inputs the model was asked to process in a single prompt, from 4 to 512.

An additional test was run using a **structured output** prompt, which required the model to first provide an analysis of the function before generating the results.
